%
% File acl2018.tex
%
%% Based on the style files for ACL-2017, with some changes, which were, in turn,
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
%\usepackage{bibtex}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.




\title{Trying Tries Algorithm}

\author{Denizhan Pak \\ Indiana University - Computational
       Linguistics Department\\ {\tt denpak@iu.edu}}

\date{October 10, 2019}

\begin{document}
\maketitle
\begin{abstract}
        There are many reasons to apply morphological analysis at the 
        sentence level. For the application of computational tools to 
        corpus data it is important that all tokens are specified to 
        allow the tools to have as informative data as possible.
        To determine the list of morphemes in a language however is a time
        consuming task and unsupervised algorithm could relieve quite a few
        researchers and grad students. In this paper we propose a 
        potentially useful unsupervised machine learning algorithm to 
        accomplish just this task.
\end{abstract}


\section{Introduction}
Morpheme parsing is an important task from a computational linguistics 
standpoint as it provides a way to denote meaningful units within a corpus
in turn this allows us to apply the many tools which use these units using the
lowest components which still provide information. We propose that smaller 
semantic units can be differentiated by their relative frequency. More 
explicitly if we are able to identify substrings which occur as a cohesive 
unit with a high relative frequency then those units correspond to morphemes 
or some other sort of semantic unit. The algorithm below provides an 
unsupervised method through which such semantic units can be distinguished.
The algorithm uses a data structure similar to a "trie" however edges between 
nodes are assigned a transition probability, we shall call this a "p-trie." 
The algorithm requires 3 user determined hyper parameters: 
$0 < \lambda $, $< 1 < \rho$, and a function $f:\mathbb{Z}\to[0,1]$. 
Where $\rho$ is a reinforcement rate, 
$\lambda$ is a learning rate, and $f$ is a probability function. (For the 
purpose of this project I will be using a translated sigmoidal function.)
\begin{algorithm}
  \label{alg:algorithm1}
\caption{Build Trie}
\begin{algorithmic}
\STATE $context \leftarrow root$
\FOR{$char$ in $corpus$}
\IF{$pointer ==  root$}
\STATE $p_1 = 1,p_2=1$
\ELSE
\STATE $p_1 = \lambda, p_2 = \rho$
\ENDIF
\IF{$char$ in $context\to children$}
\STATE $weight_{char} \leftarrow weight_{char} * p_2$
\ELSE
\STATE $weight_{char} \leftarrow p_1$
\STATE $context\to children\ \textbf{append}\ char$
\ENDIF
\STATE $r \leftarrow \textbf{random number(0,1)}$
\IF{$r > f(weight_{char})$}
\STATE $context \leftarrow root$
\ELSE
\STATE $context \leftarrow char$
\ENDIF
\ENDFOR
\STATE $\textbf{return}\ root$
\end{algorithmic}
\end{algorithm}
At the end of the algorithm the function will have returned a $p-trie$ with
the estimated probability values. Once we have a $p-trie$ it is possible to
extract potential morphemes as sequences of nodes that start at the root. We
can even assign a certainty per morpheme which is the product of the weights 
along its edges. This certainty can then be used to rate the likelihood of 
possible segmentations. If the corpus is not large enough an easy approach to 
improving access to data would simply be to randomize the words in the corpus
and run the algorithm through it again starting with the existing $p-trie$. 
This technique is similar to random walks along markov chain. Since it is a 
Bayesian method we can incorporate priors by passing a previously calculated
$p-trie$ \cite{Bayesian}.
\section{Proposed Goals}
\subsection{Minimum Viable Product}
The minimum viable product of this project is a complete implementation of the
algorithm defined in \textbf{Algorithm} \ref{alg:algorithm1} along with a 
comprehensive performance analysis. The 
analysis will include accuracy testing across multiple corpora in multiple
languages along with the effects of different parameterizations.
\subsection{Expected Product}
The expected product will be the minimum viable product as well as specific 
optimizations such as simulated annealing for the parameter values, a more
complex function for reinforcement, and an automated randomization of the 
corpus designed to improve performance.
\subsection{High-Achievement Product}
The high-achievement product will be the expected product as well as a generalization of the algorithm which could be applied to finding meaningful 
sub-sequences in any sequential data with a particular eye toward word 
embeddings. As well as a potential generative application of the generated 
$p-trie$.
\section{Requirements}
\begin{itemize}
        \item A working python implementation of the algorithm.
        \item A well documented description of the algorithm and its 
                performance.
        \item A report characterizing the algorithm including its strengths 
                and weaknesses.
\end{itemize}
\section{Timelines}
\begin{itemize}
        \item 10.15-10.22 A first implementation of the algorithm and basic
                testing using a single corpus.
        \item 10.22-10.29 Testing algorithm using varying parameter sets 
                across multiple languages and corpora.
        \item 10.29-11.06 Implementation of simulated annealing and varied
                reinforcement and corpora randomization.
        \item 11.06-11.16 Evaluation of new implementation across corpora
                and languages.
        \item 11.16-11.26 Generalization of the algorithm to any subsequence
               and evaluation of performance on different data such as word
               embeddings.
       \item 11.26-12.03 Writing final report and preparing presentation.
\end{itemize}
This does not include literature review or the writing of documentation both 
of which I intend to do as they coincide with different parts of the timeline 
above.
\section{Data Policy}
All data and code will be made available on a public git page. The code will 
be written in Python. The data for testing and evaluation will be taken from
the tree UD treebanks \cite{UDDocumentation} dataset from the following languages:
\begin{itemize}
        \item English
        \item Turkish
        \item Chinese
        \item Japanese (if time suffices)
        \item Toki Pona (if time suffices)
\end{itemize}
Finally the effectiveness of the algorithm will be compared to the morphessor \cite{Morphessor} 
python implementation.
\newpage
\section{References}
\begin{thebibliography}{9}

  \bibitem{Bayes}
  Knight, Kevin
  \textit{Bayesian Inference with Tears 
  a tutorial workbook for natural language researchers}
  \bibitem{Morphessor}
    Creutz, Mathias and Lagus, Krista,
    \textit{Unsupervised models for morpheme segmentation and morphology 
    learning}
    \bibitem{UDDocumentation}
    de Marneffe, Marie-Catherine  and
      Dozat, Timothy  and
      Silveira, Natalia  and
      Haverinen, Katri  and
      Ginter, Filip  and
      Nivre, Joakim  and
      Manning, Christopher D.,
      \textit{Universal Stanford dependencies: A cross-linguistic typology}
      \bibitem{ByteEnc}
      Gage, Philip
      \textit{A New Algorithm for Data Compression}
\end{thebibliography}
\section{Appendix}
In \textbf{Figure 1} the flow diagram is presented 
to explain the functioning of the algorithm.
% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20,
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
    \begin{figure}
           
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init) {initialize trie};
    \node [cloud, right of=init] (system) {corpus};
    \node [block, below of=init] (root) {set context to root};
    \node [block, below of=root] (terbed) {get input value $\to char$};
    \node [decision, below of=terbed] (pointer) {is pointer set to root?};
    \node [block, left of=pointer, node distance=3cm] (update) 
            {set $p_1 = p_2 = 1$};
    \node [block, right of=pointer, node distance=3.3cm] (child) 
            {set $p_1 = \lambda$ and $p_2 = \rho$};
    \node [decision, below of=pointer, node distance=3.3cm] (expected) {is 
            $char$ a child node of pointer?};
    \node [block,below of=expected, right of=expected, node distance=2.33cm] 
           (notchild) {add $char$ as child with transition probability $p_1$};
    \node [block, below of=expected, left of=expected, node distance=2.33cm] 
            (ischild) {multiply transition probability of char by $p_2$};
    
    \node [block, below of=expected, node distance=6cm] (transition) 
            {set context (probabilistically) $p_3 = f(\text{transition probability of $char$})$};
    \node [block, below of=transition, node distance=3.3cm] (input) 
            {set context to $char$};
    % Draw edges
    \path [line] (init) -- (root);
    \path [line] (root) -- (terbed);
    \path [line] (terbed) -- (pointer);
    \path [line] (pointer) -| node [near start] {yes} (update);
    \path [line] (pointer) -- node [near start] {no} (child);
    \path [line] (update) |- (expected);
    \path [line] (child) |- (expected);
    \path [line] (expected) -- node  {yes} (ischild);
    \path [line] (expected) -- node  {no} (notchild);
    \path [line,dashed] (system) |- (terbed);
    \path [line] (ischild) -- (transition);
    \path [line] (notchild) -- (transition);
    \path [line, dashed] (transition) -- node {$p_3$} (input);
    \path [line, dashed] (transition) -|++  (-5,13) node [near start]{$1-p_3$}|- (root);
    \path [line] (input) -|++  (-5,13) |- (terbed);
\end{tikzpicture}
        \label{Flow}
        \caption{Flow Diagram}
 \end{figure}

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2018}
\end{document}
